<!DOCTYPE html>
<!-- VeP http://web.unibas.it/bloisi/corsi/visione-e-percezione.html -->
<html lang="en">
<!--<![endif]-->

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Alphabet Hand Signs Recognition</title>
    <!-- Meta -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Descrizione del progetto">
    <meta name="author" content="Domenico Bloisi adapted a 3rd Wave Media template">
    <link rel="shortcut icon" href="http://web.unibas.it/bloisi/tutorial/favicon.ico">
    <link href="VeP-progetto_files/css.txt" rel="stylesheet" type="text/css">
    <link href="VeP-progetto_files/css1.txt" rel="stylesheet" type="text/css">
    <!-- Global CSS -->
    <link rel="stylesheet" href="VeP-progetto_files/bootstrap.css">
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="VeP-progetto_files/font-awesome.css">

    <!-- Theme CSS -->
    <link id="theme-style" rel="stylesheet" href="VeP-progetto_files/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>
    <!-- ******HEADER****** -->
    <header class="header">
        <div class="container">
            <img class="profile-image img-responsive pull-left"
                src="https://user-images.githubusercontent.com/42124482/72891817-62d5ae80-3d15-11ea-9a18-717771f9c33d.gif"
                alt="Inserire immagine qui" hegiht="250" width="250">
            <div class="profile-content pull-left">
                <h1 class="name">Alphabet Hand Signs Recognition</h1>
                <h2 class="desc">Detecting alphabet gestures for simplify deafs' interactions</h2>
            </div>

            <div class="profile-content pull-right">
                <img class="profile-image img-responsive pull-left"
                    src="http://web.unibas.it/bloisi/assets/images/logo.png" alt="unibas logo" height=97 width=312 />

                <p>&nbsp;</p>
                <h3 class="desc">
                    <a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html" target="_blank">
                        Corso di Visione e Percezione</a>
                </h3>
            </div>

        </div>
        <!--//container-->
    </header>
    <!--//header-->

    <div class="container sections-wrapper">
        <div class="row">
            <div class="primary col-md-8 col-sm-12 col-xs-12">

                <section class="about section" id="problem">
                    <div class="section-inner">
                        <h2 class="heading">Problem</h2>
                        <div class="content">
                            <p>
                                This project aims to classify alphabet hand gesture using a custom wrapper of MediaPipe,
                                which is a Google's library used for creating real time computer vision projects.
                            </p>
                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </section>
                <!--//section-->

                <section class="about section" id="motivation">
                    <div class="section-inner">
                        <h2 class="heading">Motivations</h2>
                        <div class="content">
                            <p>
                                We strongly belive that the modern technologies have the opportunity to enhace the every
                                day
                                live of unfortunate people.
                                The main purpose of this project is to develop a software module capable of distinguish
                                between different
                                hand signs and make possible to realize speech synthesizer application.
                            </p>
                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </section>
                <!--//section-->

                <!-- <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="goals"></a>Goals</h2>
                        <div class="content">
                            <p>

                            </p>
                        </div> -->
                <!--//content-->
                <!-- </div> -->
                <!--//section-inner-->
                <!-- </section> -->
                <!--//section-->

                <!--//section-->

                <section class="about section" id="goals">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Goals</h2>
                        <div class="content">
                            <p>Main objectives<br>
                            <ul>
                                <li>Creation of Mediapipe Wrapper</li>
                                <li>Dataset Construction</li>
                                <li>Model Realization</li>
                            </ul>
                            </p>

                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </section>

                <section class="about section" id="method">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Method</h2>
                        <div class="content">
                            <p>
                                The first problem to solve was the detection of the
                                hands: we choice <a href="https://google.github.io/mediapipe/">MediaPipe</a> to do that
                                because it provides a module that utilizes an ML pipeline consisting of multiple models
                                working together: A palm detection model that operates on the full image and returns an
                                oriented hand bounding box. A hand landmark model that operates on the cropped image
                                region defined by the palm detector and returns high-fidelity 3D hand keypoints.
                                Providing the accurately cropped hand image to the hand landmark model drastically
                                reduces the need for data augmentation and instead allows the network to dedicate most
                                of its capacity towards coordinate prediction accuracy. So we have created the wrapper
                                <b>HandTrackingModule.py</b> which is
                                capable of processing the landmarks obtained from the MediaPipe Hand Module and
                                returning data in a suitable form.
                            </p>
                            <p>
                                Secondly, we had to create the dataset with the alphabet gestures (we referenced
                                this
                                <a href="https://www.youtube.com/watch?v=0Yx9IkOxFyI" target="_blank">video</a>).
                                Since there are two dynamic gestures for Z and J we didn't include these letters in the
                                dataset. Indeed, we are capable of processing a single frame, not a sequence.
                                We have 24 possible diffetent labels + 1 because we consider an open hand gesture as a
                                space for dividing words. We build the script <b>dataset_constructor.py</b> which can be
                                used to build the example for every label.
                            </p>
                            <p>Finally, we created a model that can classifify the gesture relying on the data outputed
                                by the HandTrackingModule. As possible application we have developped a simple app
                                <b>gesture_detector.py</b> that displays the letters according to the gesture and
                                pronounce them with a speech synthesizer
                            </p>
                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </section>

                <section class="about section" id="implementation">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Implementation & Code</h2>
                        <div class="content">
                            <h4>Wrapper</h4>
                            <p>The wrapper we built has two main functions:<br>
                            <ul>
                                <li>Draw hands on an input image</li>
                                <li>Detect the landmarks for a determinate hand</li>
                            </ul>
                            </p>
                            <p>The first thing to do is to create an <b>HandTrakingModule</b> object
                                passing in input the following arguments:</p>
                            <ul>
                                <li><b>mode</b> for the type of detection we want: If we want a Static detection for
                                    single image we pass true. Otherwise if we want a Dynamic detection, like our case,
                                    for a detection using ordered frames of the same video we pass false.</li>
                                <li><b>max_hands</b> The max number of hands detected. For better results is it worth to
                                    set this parameter max to 2</li>
                                <li><b>detection_confidence</b> The percent of confidence in the first detection of the
                                    hands: Obviously higher is the confidence better detection result we have, but is
                                    less likely to find the hands.</li>
                                <li><b>track_confidence</b> The percent of confidence in the tracking of the hands frame
                                    by frame after the first detection.</li>
                            </ul>
                            <p>After the creation of the object, we can call the method <b>draw_hands_on_image</b> to
                                draw the landmark of the hands linked by green lines, passing the image as an argument.
                                The method will return the img with landmarks drawn.</p>

                            <img class="landscape-media" src="VeP-progetto_files/img/mediapipe_hands.png"
                                alt="Media Pipe Hands">

                            <p>The second main method we buildt is <b>find_position</b> which takes in input the img and
                                the hand to detect: to choose the hand to detect we can use two constants defined in the
                                wrapper:</p>
                            <ul>
                                <li><b>RIGHT_HAND</b></li>
                                <li><b>LEFT_HAND</b></li>
                            </ul>
                            <p>The method returns the landmark-list with 21 elements: every element of the list has 7
                                elements:</p>
                            <ul>
                                <li><b>id</b> described using the image below</li>
                                <li><b>3 coordinates:</b> x,y,z of the specific landmark depending on the img size</li>
                                <li><b>3 normalized coordinates:</b> x,y,z of the specific landmark between 0 and 1</li>
                            </ul>
                            <img class="landscape-media" src="VeP-progetto_files/img/mediapipe_landmarks.png"
                                alt="Media Pipe Hands">

                            <h4>Dataset Construction</h4>
                            <p>The first task to do was to create the dataset with the alphabet gestures. We referenced
                                this <a href="https://www.youtube.com/watch?v=0Yx9IkOxFyI" target="_blank">video</a>:
                            </p>
                            <p>Since there are two dynamic features for Z and J we used the other letters for the
                                algorithm. So we have 24 possible labels + 1 because we consider an open hand as a
                                space. We build the script <b>dataset_constructor.py</b> which can be used to build the
                                example for every label. In this script there are some parameters that can be set:</p>
                            <ul>
                                <li><b>num_labels</b> To choose the number of the example to build for the label. We
                                    used 500 examples for every label.</li>
                                <li><b>DETECTION_CONFIDENCE</b> The higher is detection confidence the more accurate
                                    will be the examples. The max is always 1 but is not recommended because the
                                    precision of 100% is never reached by MediaPipe. We used 0.80 as detection
                                    confidence.</li>
                                <li><b>TRACK_CONFIDENCE</b> A similar speech can be made for the track confidence. in
                                    this case we use 0.85 as track confidence.</li>
                            </ul>
                            After setting the parameters, running the script the system will require an id for the
                            label. We use the number from 0 to 24 to identify the labels. Then the script open the
                            webcam using <b>OpenCV</b> and uses the HandTrackingModule written before to detect the hand
                            and get the landmark list. The strategy adopted is to use as a feature for every example the
                            distance between the most relevant landmarks. To calculate this distance we write the
                            <b>landmark_operator</b> module wich calculate the distance using the <b>Math</b> python
                            module. Using the <b>Pandas</b> library it saves this values in a Pandas dataframe and later
                            in a file csv. We create a csv for every label and after we create the file
                            <b>alphabet_mapping.json</b> which contains the mapping from the label used [0,1, ...24] to
                            the associated character [a, b, .... ' ']. More information about the content of the entries
                            in the csv files could be found in the <a href="#dataset-section">Dataset</a> section. In
                            the following video is shown an example of dataset construction with 100 entries for the
                            letter 'a'.
                        </div>
                        <video class="landscape-media-full" controls>
                            <source src="VeP-progetto_files/video/Dataset-Construction-Example.mov" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>

                        <h4>Model</h4>
                        <p>
                            The model is based on a Logistic Regression between the 32 features. Firstly, we have
                            mounted the drive with the whole dataset and then imported the data, then we randomly split
                            the data in training set and test set respectively 80% and 20% of the whole dataset.
                            After that we have used a grid search in order to tune the algorithm with different values
                            of iteration, regularization factor and solvers.
                            During the cross validation is used the repeated stratified k-fold algorithm
                            which in this case divide the data in 10 folds and is repeated for 3 times.
                            The notebook displays the paramethers' values which got the best scores
                            during the training and the cross validation. In this way, if
                            the colab runtime is disconnected due to inactivity, we can retrain the model in just few
                            seconds instead of waiting several minutes for the tuning to be done.
                            In the end that there is a section dedicated to the valutation of the model's performance on
                            the test set. The test result includes a confusion matrix and a classification report with
                            precision, recall and f-measure for each class and it can be read in the colab preview
                            below.
                            Finally, there is a cell that serialize the model in <i>.sav</i> file so that it can be used
                            in an application.
                        </p>
                        <script
                            src="https://gist.github.com/giuseppemontanaro/066e2187ca21220dabfb80e786bf8e01.js"></script>
                        <p>
                            For the notebook used for training the model: <a
                                href="https://colab.research.google.com/drive/12k0LRJM2ubkQOYWJl8sd_wBuM2XxYE2-#scrollTo=xP4QgChYxtyQ">
                                Google Colab
                            </a>
                            <br>
                            For whole project: <a
                                href="https://github.com/giuseppemontanaro/alphabet-gesture-recognition">GitHub</a>
                        </p>
                    </div>
                </section>

                <section class="about section" id="dataset-section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Dataset</h2>
                        <div class="content">
                            <p>The csv for each letter can be found at this <a
                                    href="https://github.com/giuseppemontanaro/alphabet-gesture-recognition/tree/main/dataset">link</a>.
                            </p>
                            <p>An example of how the csv looks like is shown in the following image representing the
                                first 20
                                entries of the 'a' dataset.</p>
                            <img class="landscape-media-full" src="VeP-progetto_files/img/Dataset-example.png"
                                alt="Dataset Example">
                            <p>Every csv is divided in 32 column for the <b>features</b> and 1 column for the
                                <b>label</b> using
                                the following conventions:
                            </p>
                            <ul>
                                <li>The column from <b>0</b> to <b>19</b> represents the distance between the landmark 0
                                    associated to the palm of the hand and the others 20 landmarks.</li>
                                <li>The column from <b>20</b> to <b>23</b> represents the distance between the landmarks
                                    2, 6,
                                    10, 14, 18 associated to the <b>first</b> level of the phalanges.</li>
                                <li>The column from <b>24</b> to <b>27</b> represents the distance between the landmarks
                                    3, 7,
                                    11, 15, 19 associated to the <b>second</b> level of the phalanges.</li>
                                <li>The column from <b>28</b> to <b>31</b> represents the distance between the landmarks
                                    4, 8,
                                    12, 16, 20 associated to the <b>third</b> level of the phalanges.</li>
                                <li>The column n. <b>32</b>(y) represents the label associated to the <b>letter
                                        considered</b>.
                                </li>
                            </ul>
                            <div class="flex-row">
                                <img class="flex-element" src="VeP-progetto_files/img/landmarks-1.png"
                                    alt="landmarks-1">
                                <img class="flex-element" src="VeP-progetto_files/img/landmarks-2.png"
                                    alt="landmarks-2">
                                <img class="flex-element" src="VeP-progetto_files/img/landmarks-3.png"
                                    alt="landmarks-3">
                            </div>

                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </section>
                <!--//section-->

                <section class="about section" id="results">
                    <div class="section-inner">
                        <h2 class="heading"><a id="training"></a>Results</h2>
                        <div class="content">
                            <h3>Qualitative Results</h3>
                            <p>Below is shown an example of application's use with the sentence 'my name is Simone'</p>

                            <video class="landscape-media-full" controls>
                                <source src="VeP-progetto_files/video/Utilizzo-Applicazione.mov" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>

                            <h3>Quantitative Results</h3>
                            <p>
                                The resulting model (tuned with grid search) has these parameters:
                            <ul>
                                <li>C: 100</li>
                                <li>max_iter: 100</li>
                                <li>penalty: l2</li>
                                <li>solver: newton-cg</li>
                            </ul>
                            </p>
                            <p>
                                In general, the model developed has great performances with almost every class. In all
                                these cases, it reached f-mesure score above 0.90. Some problems have arisen in the
                                classification of the letters H, P and Q (classes number 7, 14, 15), here the f-measure
                                score are respectively 0.76, 0.81, 0.87. Sadly, the problem is mainly due to the
                                MediaPipe Hand Module which is not capable of correctly detecting the hand doing the
                                gestures of these letters. Unfortunately, for this reason we aren't able to enhace the
                                performace for these classes.
                                <br>
                                Below are shown the all values of confusion matrix and precision, recall and f-measure
                                for every classes.
                                <img class="landscape-media" src="./VeP-progetto_files/img/confusion_matrix.png"
                                    alt="confusion_matrix">
                                <img class="landscape-media" src="./VeP-progetto_files/img/classification_report.png"
                                    alt="classification_report.png">
                            </p>
                            </p>
                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </section>
                <!--//section-->

            </div>
            <!--//primary-->

            <div class="secondary col-md-4 col-sm-12 col-xs-12">
                <aside class="info aside section">
                    <div class="section-inner">
                        <h2 class="heading">Authors</h2>
                        <div class="content">
                            <p>Giuseppe Montanaro, 62719</p>
                            <p>Simone Smaldore, 62728</p>
                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </aside>
                <!--//aside-->

                <aside class="blog aside section">
                    <div class="section-inner">
                        <h2 class="heading">Table of Contents</h2>
                        <div class="content">

                            <div class="item">
                                <a href="#problem">Problem</a>
                            </div>
                            <div class="item">
                                <a href="#motivation">Motivations & Goals</a>
                            </div>
                            <div class="item">
                                <a href="#method">Method</a>
                            </div>
                            <div class="item">
                                <a href="#implementation">Implementation & Code</a>
                            </div>
                            <div class="item">
                                <a href="#dataset">Dataset</a>
                            </div>
                            <div class="item">
                                <a href="#results">Results</a>
                            </div>

                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </aside>
                <!--//section-->

                <aside class="blog aside section">
                    <div class="section-inner">
                        <h2 class="heading">Useful Links</h2>
                        <div class="content">

                            <div class="item">
                                <a href="https://web.unibas.it/bloisi/" target="_blank">Domenico Bloisi's home page</a>
                            </div>
                            <div class="item">
                                <a href="https://opencv.org/" target="_blank">OpenCV</a>
                            </div>
                            <div class="item">
                                <a href="https://mediapipe.dev/" target="_blank">MediaPipe</a>
                            </div>
                            <div class="item">
                                <a href="https://scikit-learn.org/stable/" target="_blank">scikit-learn</a>
                            </div>
                            <div class="item">
                                <a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank">Google
                                    Colaboratory</a>
                            </div>

                        </div>
                        <!--//content-->
                    </div>
                    <!--//section-inner-->
                </aside>
                <!--//section-->

            </div>
            <!--//secondary-->
        </div>
        <!--//row-->
    </div>
    <!--//masonry-->

    <!-- ******FOOTER****** -->
    <footer class="footer">
        <div class="container text-center">
            <small class="copyright">This template adapted from <a href="http://themes.3rdwavemedia.com/"
                    target="_blank">3rd Wave Media</a></small>
        </div>
        <!--//container-->
    </footer>
    <!--//footer-->





</body>

</html>